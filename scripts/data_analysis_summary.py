#!/usr/bin/env python3

import pandas as pd
import numpy as np
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

class DataAnalysisSummary:
    """TÃ³m táº¯t phÃ¢n tÃ­ch dá»¯ liá»‡u"""
    
    def __init__(self, data_path='data/Iris.csv'):
        """Khá»Ÿi táº¡o vá»›i Ä‘Æ°á»ng dáº«n dá»¯ liá»‡u"""
        self.data_path = data_path
        self.df = None
        self.numeric_features = None
        self.insights = []
        
    def load_data(self):
        """Táº£i dá»¯ liá»‡u"""
        try:
            self.df = pd.read_csv(self.data_path)
            self.numeric_features = [col for col in self.df.select_dtypes(include=[np.number]).columns.tolist() if col != 'Id']
            print(f"âœ… Dá»¯ liá»‡u Ä‘Ã£ táº£i: {self.df.shape[0]} máº«u, {self.df.shape[1]} cá»™t")
            return True
        except Exception as e:
            print(f"âŒ Lá»—i khi táº£i dá»¯ liá»‡u: {e}")
            return False
    
    def analyze_data_quality(self):
        """PhÃ¢n tÃ­ch cháº¥t lÆ°á»£ng dá»¯ liá»‡u"""
        print("\n" + "="*60)
        print("ğŸ” PHÃ‚N TÃCH CHáº¤T LÆ¯á»¢NG Dá»® LIá»†U")
        print("="*60)
        
        # Missing values
        missing_count = self.df.isnull().sum().sum()
        if missing_count == 0:
            print("âœ… KhÃ´ng cÃ³ missing values")
            self.insights.append("Dá»¯ liá»‡u sáº¡ch, khÃ´ng cÃ³ missing values")
        else:
            print(f"âš ï¸ CÃ³ {missing_count} missing values")
            self.insights.append(f"CÃ³ {missing_count} missing values cáº§n xá»­ lÃ½")
        
        # Duplicates
        duplicates = self.df.duplicated().sum()
        if duplicates == 0:
            print("âœ… KhÃ´ng cÃ³ dá»¯ liá»‡u trÃ¹ng láº·p")
            self.insights.append("KhÃ´ng cÃ³ dá»¯ liá»‡u trÃ¹ng láº·p")
        else:
            print(f"âš ï¸ CÃ³ {duplicates} dÃ²ng trÃ¹ng láº·p")
            self.insights.append(f"CÃ³ {duplicates} dÃ²ng trÃ¹ng láº·p")
        
        # Data types
        print(f"ğŸ“Š Kiá»ƒu dá»¯ liá»‡u:")
        for col, dtype in self.df.dtypes.items():
            print(f"  {col}: {dtype}")
        
        # Value ranges
        print(f"\nğŸ“ˆ Pháº¡m vi giÃ¡ trá»‹:")
        for feature in self.numeric_features:
            min_val = self.df[feature].min()
            max_val = self.df[feature].max()
            print(f"  {feature}: {min_val:.2f} - {max_val:.2f}")
    
    def analyze_distribution(self):
        """PhÃ¢n tÃ­ch phÃ¢n bá»‘ dá»¯ liá»‡u"""
        print("\n" + "="*60)
        print("ğŸ“Š PHÃ‚N TÃCH PHÃ‚N Bá» Dá»® LIá»†U")
        print("="*60)
        
        # Class distribution
        species_counts = self.df['Species'].value_counts()
        print("ğŸŒ¸ PhÃ¢n bá»‘ cÃ¡c loÃ i:")
        for species, count in species_counts.items():
            percentage = count / len(self.df) * 100
            print(f"  {species}: {count} máº«u ({percentage:.1f}%)")
        
        if len(species_counts) == len(species_counts.unique()):
            print("âœ… PhÃ¢n bá»‘ cÃ¢n báº±ng giá»¯a cÃ¡c loÃ i")
            self.insights.append("PhÃ¢n bá»‘ cÃ¢n báº±ng giá»¯a cÃ¡c loÃ i")
        else:
            print("âš ï¸ PhÃ¢n bá»‘ khÃ´ng cÃ¢n báº±ng")
            self.insights.append("PhÃ¢n bá»‘ khÃ´ng cÃ¢n báº±ng giá»¯a cÃ¡c loÃ i")
        
        # Feature distributions
        print(f"\nğŸ“ˆ Thá»‘ng kÃª mÃ´ táº£:")
        stats_df = self.df[self.numeric_features].describe()
        print(stats_df.round(3))
        
        # Normality test
        print(f"\nğŸ”¬ Kiá»ƒm Ä‘á»‹nh tÃ­nh chuáº©n (Shapiro-Wilk):")
        for feature in self.numeric_features:
            stat, p_value = stats.shapiro(self.df[feature])
            is_normal = p_value > 0.05
            status = "âœ… Chuáº©n" if is_normal else "âŒ KhÃ´ng chuáº©n"
            print(f"  {feature}: p-value={p_value:.4f} ({status})")
            
            if is_normal:
                self.insights.append(f"{feature} cÃ³ phÃ¢n bá»‘ chuáº©n")
            else:
                self.insights.append(f"{feature} khÃ´ng cÃ³ phÃ¢n bá»‘ chuáº©n")
    
    def analyze_correlations(self):
        """PhÃ¢n tÃ­ch tÆ°Æ¡ng quan"""
        print("\n" + "="*60)
        print("ğŸ”— PHÃ‚N TÃCH TÆ¯Æ NG QUAN")
        print("="*60)
        
        # Correlation matrix
        correlation_matrix = self.df[self.numeric_features].corr()
        print("ğŸ“Š Ma tráº­n tÆ°Æ¡ng quan:")
        print(correlation_matrix.round(3))
        
        # Strong correlations
        strong_correlations = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > 0.7:
                    strong_correlations.append({
                        'feature1': correlation_matrix.columns[i],
                        'feature2': correlation_matrix.columns[j],
                        'correlation': corr_value
                    })
        
        if strong_correlations:
            print(f"\nğŸ”— TÆ°Æ¡ng quan máº¡nh (|r| > 0.7):")
            for corr in strong_correlations:
                print(f"  {corr['feature1']} - {corr['feature2']}: {corr['correlation']:.3f}")
                self.insights.append(f"TÆ°Æ¡ng quan máº¡nh giá»¯a {corr['feature1']} vÃ  {corr['feature2']} ({corr['correlation']:.3f})")
        else:
            print("âœ… KhÃ´ng cÃ³ tÆ°Æ¡ng quan máº¡nh")
            self.insights.append("KhÃ´ng cÃ³ tÆ°Æ¡ng quan máº¡nh giá»¯a cÃ¡c Ä‘áº·c trÆ°ng")
    
    def analyze_outliers(self):
        """PhÃ¢n tÃ­ch outliers"""
        print("\n" + "="*60)
        print("ğŸ” PHÃ‚N TÃCH OUTLIERS")
        print("="*60)
        
        outlier_summary = []
        
        for feature in self.numeric_features:
            # Z-score method
            z_scores = np.abs(stats.zscore(self.df[feature]))
            outliers_z = len(self.df[z_scores > 3])
            
            # IQR method
            Q1 = self.df[feature].quantile(0.25)
            Q3 = self.df[feature].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers_iqr = len(self.df[(self.df[feature] < lower_bound) | (self.df[feature] > upper_bound)])
            
            outlier_summary.append({
                'feature': feature,
                'z_score_outliers': outliers_z,
                'iqr_outliers': outliers_iqr,
                'total_samples': len(self.df)
            })
            
            print(f"ğŸ“Š {feature}:")
            print(f"  Z-score outliers: {outliers_z} ({outliers_z/len(self.df)*100:.1f}%)")
            print(f"  IQR outliers: {outliers_iqr} ({outliers_iqr/len(self.df)*100:.1f}%)")
        
        # Overall assessment
        total_outliers_z = sum(item['z_score_outliers'] for item in outlier_summary)
        total_outliers_iqr = sum(item['iqr_outliers'] for item in outlier_summary)
        
        if total_outliers_z == 0:
            print("âœ… KhÃ´ng cÃ³ outliers Ä‘Ã¡ng ká»ƒ (Z-score)")
            self.insights.append("KhÃ´ng cÃ³ outliers Ä‘Ã¡ng ká»ƒ theo Z-score")
        else:
            print(f"âš ï¸ Tá»•ng cá»™ng {total_outliers_z} outliers (Z-score)")
            self.insights.append(f"CÃ³ {total_outliers_z} outliers theo Z-score")
        
        if total_outliers_iqr == 0:
            print("âœ… KhÃ´ng cÃ³ outliers Ä‘Ã¡ng ká»ƒ (IQR)")
            self.insights.append("KhÃ´ng cÃ³ outliers Ä‘Ã¡ng ká»ƒ theo IQR")
        else:
            print(f"âš ï¸ Tá»•ng cá»™ng {total_outliers_iqr} outliers (IQR)")
            self.insights.append(f"CÃ³ {total_outliers_iqr} outliers theo IQR")
    
    def analyze_separability(self):
        """PhÃ¢n tÃ­ch kháº£ nÄƒng phÃ¢n tÃ¡ch giá»¯a cÃ¡c loÃ i"""
        print("\n" + "="*60)
        print("ğŸ¯ PHÃ‚N TÃCH KHáº¢ NÄ‚NG PHÃ‚N TÃCH")
        print("="*60)
        
        # ANOVA test
        print("ğŸ”¬ Kiá»ƒm Ä‘á»‹nh ANOVA (so sÃ¡nh trung bÃ¬nh giá»¯a cÃ¡c loÃ i):")
        species_list = self.df['Species'].unique()
        
        for feature in self.numeric_features:
            groups = [self.df[self.df['Species'] == species][feature].values 
                     for species in species_list]
            f_stat, p_value = stats.f_oneway(*groups)
            
            is_significant = p_value < 0.05
            status = "âœ… CÃ³ Ã½ nghÄ©a" if is_significant else "âŒ KhÃ´ng cÃ³ Ã½ nghÄ©a"
            print(f"  {feature}: F={f_stat:.2f}, p={p_value:.4f} ({status})")
            
            if is_significant:
                self.insights.append(f"{feature} cÃ³ kháº£ nÄƒng phÃ¢n tÃ¡ch tá»‘t giá»¯a cÃ¡c loÃ i")
            else:
                self.insights.append(f"{feature} khÃ´ng cÃ³ kháº£ nÄƒng phÃ¢n tÃ¡ch tá»‘t")
        
        # Effect size (eta-squared)
        print(f"\nğŸ“Š Hiá»‡u lá»±c phÃ¢n tÃ¡ch (Effect size):")
        for feature in self.numeric_features:
            groups = [self.df[self.df['Species'] == species][feature].values 
                     for species in species_list]
            
            # Calculate eta-squared
            grand_mean = np.mean([np.mean(group) for group in groups])
            ss_between = sum(len(group) * (np.mean(group) - grand_mean)**2 for group in groups)
            ss_total = sum((val - grand_mean)**2 for group in groups for val in group)
            eta_squared = ss_between / ss_total
            
            effect_size = "Lá»›n" if eta_squared > 0.14 else "Trung bÃ¬nh" if eta_squared > 0.06 else "Nhá»"
            print(f"  {feature}: Î·Â² = {eta_squared:.3f} ({effect_size})")
    
    def analyze_dimensionality(self):
        """PhÃ¢n tÃ­ch chiá»u dá»¯ liá»‡u"""
        print("\n" + "="*60)
        print("ğŸ“‰ PHÃ‚N TÃCH CHIá»€U Dá»® LIá»†U")
        print("="*60)
        
        # PCA analysis
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(self.df[self.numeric_features])
        
        pca = PCA()
        X_pca = pca.fit_transform(X_scaled)
        
        print("ğŸ“Š Tá»· lá»‡ phÆ°Æ¡ng sai giáº£i thÃ­ch:")
        cumulative_var = np.cumsum(pca.explained_variance_ratio_)
        
        for i, (ratio, cum_var) in enumerate(zip(pca.explained_variance_ratio_, cumulative_var)):
            print(f"  PC{i+1}: {ratio:.3f} ({ratio*100:.1f}%) - TÃ­ch lÅ©y: {cum_var:.3f} ({cum_var*100:.1f}%)")
        
        # Determine optimal number of components
        optimal_components = np.argmax(cumulative_var >= 0.95) + 1
        print(f"\nğŸ¯ Sá»‘ thÃ nh pháº§n tá»‘i Æ°u (95% phÆ°Æ¡ng sai): {optimal_components}")
        
        if optimal_components <= 2:
            print("âœ… CÃ³ thá»ƒ giáº£m chiá»u dá»¯ liá»‡u hiá»‡u quáº£")
            self.insights.append(f"CÃ³ thá»ƒ giáº£m tá»« {len(self.numeric_features)} xuá»‘ng {optimal_components} chiá»u")
        else:
            print("âš ï¸ KhÃ³ giáº£m chiá»u dá»¯ liá»‡u")
            self.insights.append("KhÃ³ giáº£m chiá»u dá»¯ liá»‡u hiá»‡u quáº£")
        
        # Feature importance in PCA
        print(f"\nğŸŒŸ ÄÃ³ng gÃ³p cá»§a tá»«ng feature trong PC1:")
        for i, feature in enumerate(self.numeric_features):
            print(f"  {feature}: {pca.components_[0, i]:.3f}")
    
    def generate_recommendations(self):
        """Táº¡o khuyáº¿n nghá»‹"""
        print("\n" + "="*60)
        print("ğŸ’¡ KHUYáº¾N NGHá»Š")
        print("="*60)
        
        recommendations = []
        
        # Data quality recommendations
        if "missing values" not in " ".join(self.insights).lower():
            recommendations.append("âœ… Dá»¯ liá»‡u sáº¡ch, khÃ´ng cáº§n xá»­ lÃ½ missing values")
        else:
            recommendations.append("âš ï¸ Cáº§n xá»­ lÃ½ missing values trÆ°á»›c khi phÃ¢n tÃ­ch")
        
        # Feature engineering recommendations
        if any("tÆ°Æ¡ng quan máº¡nh" in insight for insight in self.insights):
            recommendations.append("ğŸ”§ CÃ³ thá»ƒ táº¡o features má»›i tá»« cÃ¡c Ä‘áº·c trÆ°ng tÆ°Æ¡ng quan máº¡nh")
        
        # Model recommendations
        if any("phÃ¢n tÃ¡ch tá»‘t" in insight for insight in self.insights):
            recommendations.append("ğŸ¯ CÃ¡c Ä‘áº·c trÆ°ng cÃ³ kháº£ nÄƒng phÃ¢n tÃ¡ch tá»‘t, phÃ¹ há»£p cho classification")
        
        if any("giáº£m tá»«" in insight for insight in self.insights):
            recommendations.append("ğŸ“‰ CÃ³ thá»ƒ sá»­ dá»¥ng PCA Ä‘á»ƒ giáº£m chiá»u dá»¯ liá»‡u")
        
        # Algorithm recommendations
        if "phÃ¢n bá»‘ chuáº©n" in " ".join(self.insights):
            recommendations.append("ğŸ“Š Dá»¯ liá»‡u cÃ³ phÃ¢n bá»‘ chuáº©n, phÃ¹ há»£p vá»›i cÃ¡c thuáº­t toÃ¡n parametric")
        else:
            recommendations.append("ğŸ“Š Dá»¯ liá»‡u khÃ´ng chuáº©n, nÃªn sá»­ dá»¥ng thuáº­t toÃ¡n non-parametric")
        
        # Validation recommendations
        if "cÃ¢n báº±ng" in " ".join(self.insights):
            recommendations.append("âš–ï¸ PhÃ¢n bá»‘ cÃ¢n báº±ng, cÃ³ thá»ƒ sá»­ dá»¥ng accuracy lÃ m metric chÃ­nh")
        else:
            recommendations.append("âš–ï¸ PhÃ¢n bá»‘ khÃ´ng cÃ¢n báº±ng, nÃªn sá»­ dá»¥ng F1-score hoáº·c precision/recall")
        
        # Print recommendations
        for i, rec in enumerate(recommendations, 1):
            print(f"{i}. {rec}")
    
    def create_summary_report(self):
        """Táº¡o bÃ¡o cÃ¡o tÃ³m táº¯t"""
        print("\n" + "="*60)
        print("ğŸ“‹ BÃO CÃO TÃ“M Táº®T")
        print("="*60)
        
        # Basic statistics
        print(f"ğŸ“Š Thá»‘ng kÃª cÆ¡ báº£n:")
        print(f"  â€¢ Tá»•ng sá»‘ máº«u: {len(self.df)}")
        print(f"  â€¢ Sá»‘ Ä‘áº·c trÆ°ng: {len(self.numeric_features)}")
        print(f"  â€¢ Sá»‘ loÃ i: {len(self.df['Species'].unique())}")
        print(f"  â€¢ Missing values: {self.df.isnull().sum().sum()}")
        print(f"  â€¢ Duplicates: {self.df.duplicated().sum()}")
        
        # Key insights
        print(f"\nğŸ” Insights chÃ­nh:")
        for i, insight in enumerate(self.insights[:10], 1):  # Top 10 insights
            print(f"  {i}. {insight}")
        
        # Data quality score
        quality_score = 0
        if self.df.isnull().sum().sum() == 0:
            quality_score += 25
        if self.df.duplicated().sum() == 0:
            quality_score += 25
        if len(self.df['Species'].value_counts()) == len(self.df['Species'].unique()):
            quality_score += 25
        if any("phÃ¢n tÃ¡ch tá»‘t" in insight for insight in self.insights):
            quality_score += 25
        
        print(f"\nğŸ“ˆ Äiá»ƒm cháº¥t lÆ°á»£ng dá»¯ liá»‡u: {quality_score}/100")
        
        if quality_score >= 80:
            print("âœ… Dá»¯ liá»‡u cháº¥t lÆ°á»£ng cao, phÃ¹ há»£p cho machine learning")
        elif quality_score >= 60:
            print("âš ï¸ Dá»¯ liá»‡u cháº¥t lÆ°á»£ng trung bÃ¬nh, cáº§n má»™t sá»‘ xá»­ lÃ½")
        else:
            print("âŒ Dá»¯ liá»‡u cháº¥t lÆ°á»£ng tháº¥p, cáº§n xá»­ lÃ½ nhiá»u")
        
        # ML readiness
        print(f"\nğŸ¤– Sáºµn sÃ ng cho Machine Learning:")
        readiness_factors = []
        
        if self.df.isnull().sum().sum() == 0:
            readiness_factors.append("Dá»¯ liá»‡u sáº¡ch")
        if len(self.df['Species'].value_counts()) == len(self.df['Species'].unique()):
            readiness_factors.append("PhÃ¢n bá»‘ cÃ¢n báº±ng")
        if any("phÃ¢n tÃ¡ch tá»‘t" in insight for insight in self.insights):
            readiness_factors.append("Äáº·c trÆ°ng phÃ¢n tÃ¡ch tá»‘t")
        if len(self.df) >= 100:
            readiness_factors.append("Äá»§ dá»¯ liá»‡u")
        
        if len(readiness_factors) >= 3:
            print("âœ… Sáºµn sÃ ng cho machine learning")
            for factor in readiness_factors:
                print(f"  âœ“ {factor}")
        else:
            print("âš ï¸ Cáº§n cáº£i thiá»‡n trÆ°á»›c khi Ã¡p dá»¥ng machine learning")
    
    def run_complete_analysis(self):
        """Cháº¡y phÃ¢n tÃ­ch toÃ n diá»‡n"""
        if not self.load_data():
            return
        
        print("="*80)
        print("ğŸ“Š PHÃ‚N TÃCH Dá»® LIá»†U IRIS - TÃ“M Táº®T TOÃ€N DIá»†N")
        print("="*80)
        
        # Thá»±c hiá»‡n cÃ¡c phÃ¢n tÃ­ch
        self.analyze_data_quality()
        self.analyze_distribution()
        self.analyze_correlations()
        self.analyze_outliers()
        self.analyze_separability()
        self.analyze_dimensionality()
        self.generate_recommendations()
        self.create_summary_report()
        
        print("\n" + "="*80)
        print("âœ… PHÃ‚N TÃCH HOÃ€N THÃ€NH!")
        print("="*80)
        print("ğŸ“ CÃ¡c file Ä‘Ã£ táº¡o:")
        print("  â€¢ visualizations/ - ThÆ° má»¥c chá»©a biá»ƒu Ä‘á»“ tÄ©nh")
        print("  â€¢ interactive_report.html - BÃ¡o cÃ¡o tÆ°Æ¡ng tÃ¡c")
        print("  â€¢ visualizations/README.md - BÃ¡o cÃ¡o chi tiáº¿t")

def main():
    """HÃ m chÃ­nh"""
    analyzer = DataAnalysisSummary()
    analyzer.run_complete_analysis()

if __name__ == "__main__":
    main() 